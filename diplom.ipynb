{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57155, 2) merged start texts count\n",
      "(36660, 2) merged after delete Nans in key_skills\n",
      "{'продаж': 11052, 'управлен': 9187, 'sql': 8793, 'работ': 6195, 'переговор': 4235, 'команд': 4171, 'linux': 4120, 'пк': 4008, 'навык': 3881, 'язык': 3762, 'делов': 3696, 'английск': 3475, 'git': 3454, 'грамотн': 3369, 'клиент': 3293, 'java': 3260, 'html': 3222, 'проект': 3217, 'javascript': 3185, 'анализ': 3183, 'веден': 3008, 'настройк': 2985, 'администрирован': 2953, 'разработк': 2831, 'техническ': 2771, 'windows': 2771, 'css': 2687, 'развит': 2602, 'реч': 2452, 'презентац': 2405, 'пользовател': 2397, 'python': 2375, 'активн': 2335, 'бизнес': 2251, 'предприят': 2244, 'баз': 2236, 'дан': 2148, 'тестирован': 2131, 'project': 2064, 'информацион': 2045, 'php': 2040, 'телефон': 2038, 'обучен': 1912, 'программирован': 1909, 'management': 1894, 'postgresql': 1807, 'сетев': 1790, 'framework': 1752, 'сервер': 1722, 'server': 1711, 'mysql': 1709, 'интернет': 1696, 'проведен': 1689, 'oracle': 1638, 'adobe': 1621, 'систем': 1598, 'atlassian': 1590, 'холодн': 1559, 'общен': 1507, 'поиск': 1500, 'реклам': 1460, 'ооп': 1443, 'c++': 1429, 'google': 1411, 'crm': 1405, 'c#': 1394, 'технолог': 1387, 'оптимизац': 1377, 'оборудован': 1323, 'аналитическ': 1319, 'sap': 1314, 'jira': 1311, 'переписк': 1309, 'бухгалтер': 1291, 'мышлен': 1280, 'сайт': 1273, 'системн': 1272, 'net': 1263, 'персонал': 1262, 'seo': 1261, 'планирован': 1230, 'процесс': 1221, 'документооборот': 1203, 'привлечен': 1197, 'marketing': 1189, 'коммуникац': 1181, 'задан': 1175, 'торговл': 1126, 'информац': 1126, 'яндекс': 1052, 'photoshop': 1051, 'документац': 1050, 'ip': 1017, 'tcp': 994, 'js': 991, 'консультирован': 991, 'подключен': 980, 'erp': 970, 'организаторск': 942, 'visio': 936, 'ориентац': 936, 'договор': 920, 'зарплат': 912, 'клиентоориентирован': 907, 'аналитик': 901, 'powerpoint': 897, 'android': 892, 'безопасн': 864, 'прям': 859, 'jquery': 856, 'проектн': 825, 'analytics': 824, 'подготовк': 822, 'битрикс': 814, 'результат': 805, 'spring': 793, 'автоматизац': 792, 'agile': 779, 'звонк': 774, 'xml': 768, 'заключен': 764, 'маркетингов': 764, 'react': 743, 'дизайн': 728, 'умен': 722, 'internet': 718, 'персона': 716, 'ремонт': 715, 'сет': 707, 'маркетинг': 698, 'scrum': 683, 'поддержк': 651, 'web': 649, 'pl': 643, 'метрик': 639, 'ios': 633, 'bash': 627, 'контекстн': 622, 'docker': 621, 'интерфейс': 619, 'rest': 605, 'веб': 599, 'разработчик': 592, 'сопровожден': 589, 'больш': 583, 'excel': 581, 'продукт': 577, 'моделирован': 571, 'node': 566, 'объем': 565, 'api': 560, 'обслуживан': 554, 'копирайтинг': 550, 'uml': 537, 'skills': 534, 'active': 534, 'directory': 531, 'работа': 526, 'os': 525, 'продвижен': 516, 'отчетн': 515, 'data': 514, 'написан': 508, 'исследован': 506, 'функциональн': 506, 'asp': 502, 'ауд': 496, 'производствен': 494, 'креативн': 491, 'задач': 487, 'пользовательск': 479, 'проектирован': 478, 'организац': 476, 'apache': 476, 'angularjs': 475, 'ответствен': 470, 'постановк': 469, 'монтаж': 464, 'субд': 463, 'outlook': 460, 'visual': 458, 'access': 452, 'studio': 448, 'электрон': 443, 'nginx': 442, 'smm': 434, 'business': 433, 'mongodb': 430, 'социальн': 423, 'составлен': 418, 'менеджмент': 414, 'коллектив': 414, 'руководств': 411, 'bpmn': 409, 'мобильн': 408, 'директ': 408, 'поисков': 407, 'unix': 401, 'cisco': 396, 'adwords': 386, 'autocad': 382, 'helpdesk': 382, 'склад': 378, 'средств': 375, 'контрол': 374, 'защит': 374, 'бренд': 365, 'confluence': 360, 'cистем': 359, 'управленческ': 359, 'cms': 357, 'product': 356, 'unity': 355, 'office': 354, 'vmware': 353, 'отношен': 353, 'производств': 351, 'учет': 350, 'контент': 348, 'development': 346, 'ui': 345, 'ux': 344, 'dns': 342, 'vue': 340, 'техник': 340, 'сист': 338, 'mac': 326, 'знан': 325, 'orm': 323, 'инженерн': 322, 'наполнен': 321, 'qa': 319, 'ajax': 318, 'конфигурац': 316, 'сборк': 316, 'текст': 314, 'возражен': 314, 'hibernate': 309, 'zabbix': 308, 'ключев': 306, 'коммуникабельн': 303, 'se': 302, 'mvc': 301, 'бухгалтерск': 296, 'групп': 295, 'внутрен': 294, 'soap': 290, 'kubernetes': 288, 'риск': 288, 'django': 286, 'illustrator': 285, 'dhcp': 285, 'обновлен': 282, 'математическ': 280, 'видеонаблюден': 276, 'redis': 274, 'коммерческ': 273, 'typescript': 267, 'предложен': 267, 'розниц': 266, 'комплексн': 266, 'http': 266, 'кампан': 265, 'ci': 263, 'корпоративн': 262, 'bootstrap': 261, 'yii': 260, 'testing': 260, 'design': 258, 'ee': 258, 'sales': 258, 'jenkins': 256, 'ruby': 256, 'test': 256, 'objective': 253, 'многозадачн': 253, 'sdk': 252, 'analysis': 252, 'json': 250, 'офисн': 249, 'microsoft': 248, 'transact': 244, 'документальн': 243, 'swift': 240, 'kotlin': 239, 'мероприят': 237, 'cd': 234, 'delphi': 233, 'интеграц': 233, 'нов': 231, 'мотивац': 230, 'bi': 230, 'дистанцион': 230, 'времен': 229, 'xp': 228, 'оргтехник': 228, 'case': 221, 'стратегическ': 219, 'закупк': 219, 'media': 218}\n",
      "length of dictionary is:  300\n",
      "(300, 10)\n",
      "\n",
      "smm internet adwords кампан директ социальн аналитик продвижен метрик analytics\n",
      "pl java postgresql c# python framework net oracle server linux\n",
      "unity sdk kotlin ruby redis objective office kubernetes ci cd\n",
      "продаж\n",
      "сетев windows администрирован сервер настройк пк\n",
      "переговор клиент навык\n",
      "сайт\n",
      "реч грамотн переписк общен команд пользовател работ делов\n",
      "разработчик проектн visio задач постановк моделирован документац мышлен sap задан\n",
      "пользовательск test case qa интерфейс jira функциональн atlassian дизайн\n",
      "тестирован\n",
      "поиск привлечен договор прям развит заключен презентац проведен веден активн\n",
      "управлен\n",
      "торговл программирован зарплат персонал бухгалтер производствен erp документооборот предприят\n",
      "бизнес анализ процесс\n",
      "php javascript jquery mysql git js css html adobe\n",
      "результат сопровожден ориентац подготовк отношен возражен организац клиентоориентирован умен outlook\n",
      "ауд поисков оптимизац seo\n",
      "обслуживан поддержк техник сборк офисн защит dns tcp ip dhcp\n",
      "google яндекс интернет реклам marketing\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10 = SVD components count\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "from settings import stopwords, stem, clusters_count, main_dict_words_count, components_count\n",
    "stopwords.append('это')\n",
    "stopwords.append('наш')\n",
    "stopwords.append('лет')\n",
    "stopwords.append('тебя')\n",
    "stopwords.append('ms')\n",
    "\n",
    "\n",
    "def clear_doc(row):\n",
    "    html = ' ' + row['key_skills']\n",
    "    p = re.compile(r'<.*?>')\n",
    "    text = p.sub('', html)\n",
    "    return text\n",
    "\n",
    "\n",
    "def full_for_tf_correct(row):\n",
    "    all = row.clean_doc.split()\n",
    "    line_for_tf = 'fuck ' * 10\n",
    "    if len(all) < 3:\n",
    "        return row.clean_doc + line_for_tf\n",
    "    return row.clean_doc\n",
    "\n",
    "\n",
    "def get_cleared_key_skills(kek):\n",
    "    need_values = kek[['key_skills']]\n",
    "    print(need_values)\n",
    "    need_values.key_skills = need_values.key_skills.fillna('')\n",
    "    need_values['clean_doc'] = need_values.apply(clear_doc, axis=1)\n",
    "    need_values['clean_doc'] = need_values['clean_doc'].apply(lambda x: x.lower())\n",
    "    need_values['clean_doc'] = need_values['clean_doc'].str.replace(\"[^a-zа-яё+#]\", \" \")\n",
    "    need_values['clean_doc'] = need_values['clean_doc'].apply(\n",
    "        lambda x: ' ' + ' '.join([w for w in x.split() if len(w) >= 2]))\n",
    "\n",
    "    print('get_cleared_description end')\n",
    "    return need_values\n",
    "\n",
    "\n",
    "def get_tokenized_doc(need_values):\n",
    "    # need_values.clean_doc = need_values.apply(full_for_tf_correct, axis=1)\n",
    "    tokenized_doc = need_values['clean_doc'].apply(lambda x: x.split())\n",
    "    print(need_values.shape)\n",
    "    need_values = need_values.dropna()\n",
    "    print(need_values.shape)\n",
    "    stemmer = SnowballStemmer(stem)\n",
    "    print(stemmer.stem('приглашаем специалиста должность junior php разработчика требуется знание php git windows os умение работать команде'))\n",
    "    # remove stop-words\n",
    "    tokenized_doc = tokenized_doc.apply(\n",
    "        lambda x: ' ' + ' '.join([stemmer.stem(item).lower() for item in x if stemmer.stem(item).lower() not in stopwords]))\n",
    "\n",
    "    need_values['clean_doc'] = tokenized_doc\n",
    "    print('get_tokenized_doc end')\n",
    "    return need_values\n",
    "\n",
    "\n",
    "def get_all_words_dict_frequency(need_values, count_of_main_components):\n",
    "    dict = {}\n",
    "    for line in need_values.clean_doc:\n",
    "        for word in line.split():\n",
    "            if word in dict:\n",
    "                dict[word] += 1\n",
    "            else:\n",
    "                dict[word] = 0\n",
    "    kek = {k: v for k, v in sorted(dict.items(), key=lambda kv: kv[1], reverse=True)[:count_of_main_components]}\n",
    "    print(kek)\n",
    "    print('length of dictionary is: ', len(kek))\n",
    "    return kek\n",
    "\n",
    "\n",
    "def get_all_words_by_docs_count_dict(need_values, words):\n",
    "    dict = {word: 0 for word in words}\n",
    "    for line in need_values.clean_doc:\n",
    "        kek = line.split()\n",
    "        for word in words:\n",
    "            if word in kek:\n",
    "                dict[word] += 1\n",
    "\n",
    "    return dict\n",
    "\n",
    "\n",
    "def make_tf_idf_matrix(need_values, main_dict, main_dict_by_docs):\n",
    "    kek = []\n",
    "    all_count = []\n",
    "    freq_log = np.hstack([len(need_values.clean_doc) / main_dict_by_docs[word] for word in main_dict])\n",
    "    for line in need_values.clean_doc:\n",
    "        freq = {k: 0 for k in main_dict}\n",
    "        for word in line.split():\n",
    "            if word in freq:\n",
    "                freq[word] += 1\n",
    "\n",
    "        main_sum = sum(freq.values())\n",
    "        if main_sum == 0:\n",
    "            continue\n",
    "        all_count.append(main_sum)\n",
    "        kek.append(list(freq.values()))\n",
    "    return np.vstack(kek) * np.log(freq_log)\n",
    "\n",
    "\n",
    "def get_squared_evcklid_dist(row1, row2):\n",
    "    return np.sum((row1 - row2) ** 2)\n",
    "\n",
    "\n",
    "def get_sorted_words_by_evcklid(main_dict_words, component_labels, centres, words_count, rows):\n",
    "    all = []\n",
    "    for i in range(clusters_count):\n",
    "        d = {main_dict_words[j]: rows[j] for j in range(len(component_labels)) if component_labels[j] == i}\n",
    "        s = sorted(d.items(), key=lambda x: get_squared_evcklid_dist(x[1], centres[i]))\n",
    "        mapped = [get_squared_evcklid_dist(x[1], centres[i]) for x in s]\n",
    "        print(' '.join([elem[0] for elem in s][:words_count]))\n",
    "        # print(mapped[:words_count])\n",
    "        all.append([elem[0] for elem in s][:words_count])\n",
    "    return all\n",
    "\n",
    "\n",
    "def get_words_lists(main_dict_words, component_labels):\n",
    "    all = []\n",
    "    for i in range(clusters_count):\n",
    "        d = [main_dict_words[j] for j in range(len(component_labels)) if component_labels[j] == i]\n",
    "        print(\"dictionary words count\")\n",
    "        print(d)\n",
    "        all.append(d)\n",
    "    return all\n",
    "\n",
    "\n",
    "def make_k_means_clusterization(X, n_components):\n",
    "    kmeans = KMeans(n_clusters=n_components, random_state=0).fit(X)\n",
    "    return kmeans.labels_, kmeans.cluster_centers_\n",
    "\n",
    "\n",
    "def filter_by_labels(x, y, val, w_labels):\n",
    "    x1 = np.hstack(x)\n",
    "    y1 = np.hstack(y)\n",
    "    return (np.vstack([x1[i] for i, v in enumerate(w_labels) if v == val]),\n",
    "            np.vstack([y1[i] for i, v in enumerate(w_labels) if v == val]))\n",
    "\n",
    "\n",
    "def make_vectorize(need_values, max_features, n_iter):\n",
    "    main_dict = get_all_words_dict_frequency(need_values, max_features)\n",
    "    main_dict_by_docs = get_all_words_by_docs_count_dict(need_values, list(main_dict.keys()))\n",
    "    tf_idf = make_tf_idf_matrix(need_values, main_dict, main_dict_by_docs)\n",
    "    svd_model = TruncatedSVD(n_components=components_count, n_iter=n_iter)\n",
    "\n",
    "    kek = svd_model.fit_transform(tf_idf.T)\n",
    "    print(kek.shape)\n",
    "    print()\n",
    "    labels, centres = make_k_means_clusterization(kek, clusters_count)\n",
    "    get_sorted_words_by_evcklid(list(main_dict.keys()), labels, centres, 10, kek)\n",
    "    print('\\n' * 3)\n",
    "\n",
    "    print(len(svd_model.components_), '= SVD components count')\n",
    "    terms = main_dict.keys()\n",
    "\n",
    "    return centres\n",
    "\n",
    "\n",
    "def get_sorted_by_evclid_all_centres(centres, vectors):\n",
    "    kek = []\n",
    "    for vector in vectors:\n",
    "        all = []\n",
    "        for center in centres:\n",
    "            all.append(get_squared_evcklid_dist(center, vector))\n",
    "        kek.append(all)\n",
    "    return kek\n",
    "\n",
    "\n",
    "def main():\n",
    "    # закомментированная часть, которая подготавливает данные из merged\n",
    "    # kek = pd.read_csv('merged.csv')\n",
    "    # cleared_data = get_cleared_key_skills(kek)\n",
    "    # tokenized = get_tokenized_doc(cleared_data)\n",
    "    # tokenized.to_csv('cleaned_key_skills_without_ms.csv', encoding='utf-8', index=False)\n",
    "    tokenized = pd.read_csv('cleaned_key_skills_without_ms.csv')\n",
    "    print(tokenized.shape, 'merged start texts count')\n",
    "    tokenized = tokenized.dropna()\n",
    "    print(tokenized.shape, 'merged after delete Nans in key_skills')\n",
    "\n",
    "    make_vectorize(tokenized, main_dict_words_count, 30)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
